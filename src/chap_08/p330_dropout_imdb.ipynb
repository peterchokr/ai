{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 드롭아웃을 이용한 과잉적합 해소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다운로드\n",
    "# 가장 많이 등장하는 상위 1000개의 단어만 선택해서 훈련데이터와 테스트 데이터를 다운로드\n",
    "# 제한된 데이터 사용으로 과잉적합 원인이 된다.\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 인코딩으로 변환하는 함수\n",
    "def one_hot_sequences(sequences, dimension=1000):\n",
    "    results = numpy.zeros((len(sequences), dimension))\n",
    "    for i, word_index in enumerate(sequences):\n",
    "        results[i, word_index] = 1.\n",
    "    return results\n",
    "\n",
    "train_data = one_hot_sequences(train_data)  # 과잉적합을 발생시키는 원핫 인코딩\n",
    "test_data = one_hot_sequences(test_data)    # 과잉적합을 발생시키는 원핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 모델 구축\n",
    "# 드롭아웃으로 과잉적합 해소\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(1000,)))\n",
    "model.add(tf.keras.layers.Dropout(0.5))   #바로 앞의 출력 16개에서 반을 Dropout\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))   #바로 앞의 출력 16개에서 반을 Dropout\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 훈련, 검증 데이터 전달\n",
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(test_data, test_labels),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터의 손실값과 검증 데이터의 손실값을 그래프에 출력\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\t\t# 훈련 데이터 손실값\n",
    "val_loss_values = history_dict['val_loss']\t# 검증 데이터 손실값\n",
    "acc = history_dict['accuracy']\t\t\t# 정확도\n",
    "epochs = range(1, len(acc) + 1)\t\t# 에포크 수\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss Plot')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train error', 'val error'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
